{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch import nn, Tensor\n",
    "import scipy.sparse as sparse\n",
    "import math\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import scipy as sp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global variables\n",
    "demb = 120\n",
    "dropout = 0.2\n",
    "batch_size = 16\n",
    "device = 'mps' if torch.backends.mps.is_available() else (\n",
    "    \"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# device = 'cpu'\n",
    "\n",
    "ns = 20\n",
    "nr = 12\n",
    "npairs_local = int(nr*(nr - 1)/2)\n",
    "outlier_percentage = 0.5\n",
    "noise_std = 0.2\n",
    "\n",
    "index_to_pair = {}\n",
    "counter = 0\n",
    "for i in range(nr):\n",
    "    for j in range(i+1, nr):\n",
    "        index_to_pair[counter] = (i, j)\n",
    "        counter += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_problem(ns, nr, outlier_percentage, noise_std):\n",
    "    sender_position_truth = 5*np.random.rand(ns, 3)\n",
    "    receiver_position_truth = 5*np.random.rand(nr, 3)\n",
    "    distance_truth = torch.Tensor(sp.spatial.distance.cdist(\n",
    "        sender_position_truth, receiver_position_truth))\n",
    "    npairs = int(nr*(nr-1)/2)\n",
    "    measurements = torch.zeros(ns, npairs)\n",
    "#    measurements = distance_truth.copy()\n",
    "    counter = 0\n",
    "    for i in range(nr):\n",
    "        for j in range(i+1, nr):\n",
    "            measurements[:, counter] = distance_truth[:, i] - \\\n",
    "                distance_truth[:, j]\n",
    "            counter += 1\n",
    "\n",
    "    outliers = np.random.rand(ns, npairs) < outlier_percentage\n",
    "    # Introducing outliers by either doubling or halving the measurement, could be done in better way\n",
    "    temp = np.ones((ns, npairs))\n",
    "    temp[(np.random.rand(ns, npairs) < 0.5)] = -0.5\n",
    "    measurements = measurements*temp*outliers + measurements\n",
    "    # introducing noise\n",
    "    measurements = measurements + noise_std * \\\n",
    "        np.median(distance_truth)*np.random.randn(ns, npairs)\n",
    "\n",
    "    sender_position_truth = torch.tensor(sender_position_truth)\n",
    "    receiver_position_truth = torch.tensor(receiver_position_truth)\n",
    "    # translation\n",
    "    receiver_position_truth = receiver_position_truth - \\\n",
    "        receiver_position_truth[0, :]\n",
    "    sender_position_truth = sender_position_truth - \\\n",
    "        receiver_position_truth[0, :]\n",
    "    # rotation\n",
    "    R = torch.zeros(3, 3, dtype=torch.float64)\n",
    "    R[0, :] = receiver_position_truth[1, :] / \\\n",
    "        receiver_position_truth[1, :].norm()\n",
    "    R[1, :] = receiver_position_truth[2, :] - \\\n",
    "        R[0, :]*(receiver_position_truth[2, :]@R[0, :])\n",
    "    R[1, :] = R[1, :]/R[1, :].norm()\n",
    "    R[2, :] = torch.cross(R[0, :], R[1, :])\n",
    "    receiver_position_truth = receiver_position_truth@R.T\n",
    "    sender_position_truth = sender_position_truth@R.T\n",
    "\n",
    "    return (measurements, sender_position_truth, receiver_position_truth, 1-outliers)\n",
    "\n",
    "# prob = generate_problem(20,12,0.3,0.2)\n",
    "# prob[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getValueEncoding(value_to_encode, d):\n",
    "    max_v = max(value_to_encode)\n",
    "    denominator = max_v/np.pi/np.power(1.2, [i for i in range(int(d/2))])\n",
    "    sin_coeff = np.sin(np.expand_dims(value_to_encode, 1) /\n",
    "                       np.expand_dims(denominator, 0))\n",
    "    cos_coeff = np.cos(np.expand_dims(value_to_encode, 1) /\n",
    "                       np.expand_dims(denominator, 0))\n",
    "    return np.concatenate([sin_coeff, cos_coeff], 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class SelfAttentionHead(nn.Module):\n",
    "    def __init__(self, head_size):\n",
    "        super().__init__()\n",
    "        self.key = nn.Linear(demb, head_size, bias=False)\n",
    "        self.query = nn.Linear(demb, head_size, bias=False)\n",
    "        self.value = nn.Linear(demb, head_size, bias=False)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.head_size = head_size\n",
    "#        self.ffwd = nn.Sequential(\n",
    "#            nn.Linear(head_size,head_size*2),\n",
    "#            nn.ReLU(),\n",
    "#            nn.Linear(head_size*2,head_size)\n",
    "#            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        k = self.key(x)\n",
    "        q = self.query(x)\n",
    "        v = self.value(x)\n",
    "\n",
    "        wei = (k @ q.transpose(2, 1)) * self.head_size**-0.5\n",
    "\n",
    "        wei = F.softmax(wei, dim=-1)\n",
    "        wei = self.dropout(wei)\n",
    "        res = wei @ v\n",
    "        return res\n",
    "\n",
    "\n",
    "class MultiAttentionHead(nn.Module):\n",
    "\n",
    "    def __init__(self, head_size, n_heads):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList(\n",
    "            [SelfAttentionHead(head_size) for _ in range(n_heads)])\n",
    "        self.proj = nn.Linear(n_heads*head_size, demb)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.cat([h(x) for h in self.heads], dim=-1)\n",
    "        return self.dropout(self.proj(x))\n",
    "\n",
    "\n",
    "class Block(nn.Module):\n",
    "\n",
    "    def __init__(self, demb, n_heads):\n",
    "        super().__init__()\n",
    "        head_size = demb // n_heads\n",
    "        self.heads = MultiAttentionHead(head_size, n_heads)\n",
    "        self.ffwd = nn.Sequential(\n",
    "            nn.Linear(demb, 4*demb),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4*demb, demb),\n",
    "        )\n",
    "        self.ln1 = nn.LayerNorm(demb)\n",
    "        self.ln2 = nn.LayerNorm(demb)\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.heads(self.ln1(x))\n",
    "        return x + self.dropout(self.ffwd(self.ln2(x)))\n",
    "\n",
    "\n",
    "class TransformerNetwork(nn.Module):\n",
    "\n",
    "    def __init__(self, n_layers, n_heads):\n",
    "        super().__init__()\n",
    "        # self.pos_encode = nn.Embedding(2*data_size,demb, device=device)\n",
    "        # self.tok_encode = nn.Embedding(2,demb, device=device)\n",
    "\n",
    "        self.transformer = nn.Sequential(\n",
    "            *[Block(demb, n_heads) for _ in range(n_layers)])\n",
    "        self.condense = nn.Sequential(\n",
    "            nn.Linear(demb, 2*demb),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(2*demb, 3),\n",
    "        )\n",
    "        self.ffwd = nn.Sequential(\n",
    "            nn.Linear(3960, 400),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(400, (ns+nr)*3),\n",
    "        )\n",
    "        self.sigmoid = nn.Sigmoid(),\n",
    "        self.ln1 = nn.LayerNorm(demb)\n",
    "        self.ln2 = nn.LayerNorm(3)\n",
    "        self.apply(self._init_weights)\n",
    "        # self.pos = torch.Tensor(getPositionEncoding(\n",
    "        #    seq_len=2*data_size, d=int(demb/2), n=data_size*2)).to(device)\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.unflatten = nn.Unflatten(1, (ns+nr, 3))\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # pos_emb = self.pos_encode(torch.arange(T,device=device))\n",
    "        # x = torch.cat((x.unsqueeze(2).repeat((1, 1, int(demb/2))),\n",
    "        #              self.pos.unsqueeze(0).repeat((B, 1, 1))), dim=2)\n",
    "\n",
    "        x = self.transformer(x)\n",
    "\n",
    "        x = self.ln1(x)\n",
    "        x = self.condense(x)\n",
    "        x = self.ln2(x)\n",
    "        x = self.flatten(x)\n",
    "\n",
    "        x = self.ffwd(x)\n",
    "        x = self.unflatten(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = TransformerNetwork(4, 4).to(device)\n",
    "loss_fn = torch.nn.HuberLoss()\n",
    "\n",
    "\n",
    "def train(model, loss_fn, optimizer, generate_problem, batch_size, batches_in_epoch):\n",
    "    model.train()\n",
    "    btop = 10\n",
    "    loss_summer = 0\n",
    "    for batch in range(batches_in_epoch):\n",
    "        # Compute prediction error\n",
    "\n",
    "        X = torch.zeros(batch_size, ns*npairs_local, demb)\n",
    "        y = torch.zeros(batch_size, ns+nr, 3)\n",
    "\n",
    "        for i in range(batch_size):\n",
    "            outlier_percentage_local = np.random.rand(1)*outlier_percentage\n",
    "            noise_std_local = np.random.rand(1)*noise_std\n",
    "            prob = generate_problem(\n",
    "                ns, nr, outlier_percentage_local, noise_std_local)\n",
    "            x_coord, y_coord = torch.meshgrid(torch.arange(\n",
    "                ns), torch.arange(npairs_local), indexing='ij')\n",
    "            X[i, :] = torch.concatenate([prob[0].flatten().unsqueeze(1)*torch.ones(prob[0].numel(), demb//2),\n",
    "                                        torch.Tensor(getValueEncoding(\n",
    "                                            x_coord.flatten(), d=demb/6)),\n",
    "                                        torch.Tensor(getValueEncoding(y_coord.flatten().apply_(\n",
    "                                            lambda x: index_to_pair[x][0]), d=demb/6)),\n",
    "                                        torch.Tensor(getValueEncoding(y_coord.flatten().apply_(\n",
    "                                            lambda x: index_to_pair[x][1]), d=demb/6))\n",
    "                                         ], 1)\n",
    "\n",
    "            y[i, :] = torch.cat([prob[1], prob[2]])\n",
    "\n",
    "        X = X.to(device)\n",
    "        y = y.to(device)\n",
    "\n",
    "        pred = model(X)\n",
    "\n",
    "        # print((X - y).norm(dim=2).shape)\n",
    "        loss = loss_fn(pred, y)\n",
    "\n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        loss_summer += loss.item()/btop\n",
    "        if batch % btop == btop-1:\n",
    "            loss = loss_summer\n",
    "            loss_summer = 0\n",
    "            print(f\"loss: {loss:>7f}  [{batch:>5d}/{batches_in_epoch}]\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 1.572395  [    9/1000]\n",
      "loss: 1.533993  [   19/1000]\n",
      "loss: 1.502200  [   29/1000]\n",
      "loss: 1.527058  [   39/1000]\n",
      "loss: 1.500924  [   49/1000]\n",
      "loss: 1.509917  [   59/1000]\n",
      "loss: 1.524668  [   69/1000]\n",
      "loss: 1.504204  [   79/1000]\n",
      "loss: 1.523578  [   89/1000]\n",
      "loss: 1.506509  [   99/1000]\n",
      "loss: 1.511704  [  109/1000]\n",
      "loss: 1.508141  [  119/1000]\n",
      "loss: 1.517792  [  129/1000]\n",
      "loss: 1.503021  [  139/1000]\n",
      "loss: 1.499395  [  149/1000]\n",
      "loss: 1.519017  [  159/1000]\n",
      "loss: 1.514807  [  169/1000]\n",
      "loss: 1.496973  [  179/1000]\n",
      "loss: 1.501586  [  189/1000]\n",
      "loss: 1.506149  [  199/1000]\n",
      "loss: 1.484012  [  209/1000]\n",
      "loss: 1.504206  [  219/1000]\n",
      "loss: 1.501755  [  229/1000]\n",
      "loss: 1.517225  [  239/1000]\n",
      "loss: 1.502507  [  249/1000]\n",
      "loss: 1.498120  [  259/1000]\n",
      "loss: 1.497862  [  269/1000]\n",
      "loss: 1.468768  [  279/1000]\n",
      "loss: 1.490078  [  289/1000]\n",
      "loss: 1.491009  [  299/1000]\n",
      "loss: 1.502275  [  309/1000]\n",
      "loss: 1.490367  [  319/1000]\n",
      "loss: 1.498806  [  329/1000]\n",
      "loss: 1.500893  [  339/1000]\n",
      "loss: 1.497147  [  349/1000]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/eriktegler/phd_studies/Forskning/generated_data_learning/slask/training_network_first_test.ipynb Cell 8\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/eriktegler/phd_studies/Forskning/generated_data_learning/slask/training_network_first_test.ipynb#W5sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39mfor\u001b[39;00m t \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(epochs):\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/eriktegler/phd_studies/Forskning/generated_data_learning/slask/training_network_first_test.ipynb#W5sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m     \u001b[39m# if t % 100 == 0:\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/eriktegler/phd_studies/Forskning/generated_data_learning/slask/training_network_first_test.ipynb#W5sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m     \u001b[39m# torch.save(model, \"model\" + str(t))\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/eriktegler/phd_studies/Forskning/generated_data_learning/slask/training_network_first_test.ipynb#W5sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mEpoch \u001b[39m\u001b[39m{\u001b[39;00mt\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m-------------------------------\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/eriktegler/phd_studies/Forskning/generated_data_learning/slask/training_network_first_test.ipynb#W5sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m     train(model, loss_fn, optimizer, generate_problem, batch_size, \u001b[39m1000\u001b[39;49m)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/eriktegler/phd_studies/Forskning/generated_data_learning/slask/training_network_first_test.ipynb#W5sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m     \u001b[39m# test(test_dataloader, model, loss_fn)\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/eriktegler/phd_studies/Forskning/generated_data_learning/slask/training_network_first_test.ipynb#W5sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mDone!\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[1;32m/Users/eriktegler/phd_studies/Forskning/generated_data_learning/slask/training_network_first_test.ipynb Cell 8\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, loss_fn, optimizer, generate_problem, batch_size, batches_in_epoch)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/eriktegler/phd_studies/Forskning/generated_data_learning/slask/training_network_first_test.ipynb#W5sZmlsZQ%3D%3D?line=40'>41</a>\u001b[0m \u001b[39m# Backpropagation\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/eriktegler/phd_studies/Forskning/generated_data_learning/slask/training_network_first_test.ipynb#W5sZmlsZQ%3D%3D?line=41'>42</a>\u001b[0m optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/eriktegler/phd_studies/Forskning/generated_data_learning/slask/training_network_first_test.ipynb#W5sZmlsZQ%3D%3D?line=42'>43</a>\u001b[0m loss\u001b[39m.\u001b[39;49mbackward()\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/eriktegler/phd_studies/Forskning/generated_data_learning/slask/training_network_first_test.ipynb#W5sZmlsZQ%3D%3D?line=43'>44</a>\u001b[0m optimizer\u001b[39m.\u001b[39mstep()\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/eriktegler/phd_studies/Forskning/generated_data_learning/slask/training_network_first_test.ipynb#W5sZmlsZQ%3D%3D?line=44'>45</a>\u001b[0m loss_summer \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m loss\u001b[39m.\u001b[39mitem()\u001b[39m/\u001b[39mbtop\n",
      "File \u001b[0;32m~/miniforge3/lib/python3.9/site-packages/torch/_tensor.py:487\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    477\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[1;32m    478\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    479\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[1;32m    480\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    485\u001b[0m         inputs\u001b[39m=\u001b[39minputs,\n\u001b[1;32m    486\u001b[0m     )\n\u001b[0;32m--> 487\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\n\u001b[1;32m    488\u001b[0m     \u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs\n\u001b[1;32m    489\u001b[0m )\n",
      "File \u001b[0;32m~/miniforge3/lib/python3.9/site-packages/torch/autograd/__init__.py:200\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    195\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[1;32m    197\u001b[0m \u001b[39m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    198\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    199\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 200\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    201\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[1;32m    202\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "epochs = 5000\n",
    "model.to(device)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=3e-4)\n",
    "for t in range(epochs):\n",
    "    # if t % 100 == 0:\n",
    "    # torch.save(model, \"model\" + str(t))\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    train(model, loss_fn, optimizer, generate_problem, batch_size, 1000)\n",
    "    # test(test_dataloader, model, loss_fn)\n",
    "print(\"Done!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
